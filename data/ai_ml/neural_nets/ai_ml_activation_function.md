### Activation function

- Nonlinearity (ReLU, sigmoid, tanh) enabling expressive nets.
