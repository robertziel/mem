### Cross-entropy

- Expected âˆ’log q under true p; common classification loss.
- Practical perspective: how Cross-entropy is used day-to-day in modeling, data, or evaluation workflows.
- Pros/cons: strengths like simplicity or stability versus tradeoffs like bias, variance, or compute cost depending on the setting.
- Alternatives/related: common neighboring concepts or replacement approaches that solve similar problems in different ways.
